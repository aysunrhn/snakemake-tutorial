{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use Snakemake to analyze Airbnb data from New York City in 2019. Our pipeline includes preprocessing data, performing basic analysis, and generating visualizations. \n",
    "\n",
    "You can find the dataset in the github repository, or you can download it directly from [Kaggle here](https://www.kaggle.com/datasets/ptoscano230382/air-bnb-ny-2019).\n",
    "\n",
    "The dataset contains Airbnb listings in New York for 2019; it describes each accommodation based on its host name, neighborhood, geographical location, price, reviews and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,name,host_id,host_name,neighbourhood_group,neighbourhood,latitude,longitude,room_type,price,minimum_nights,number_of_reviews,last_review,reviews_per_month,calculated_host_listings_count,availability_365\n",
      "2539,Clean & quiet apt home by the park,2787,John,Brooklyn,Kensington,40.64749,-73.97237,Private room,149,1,9,2018-10-19,0.21,6,365\n",
      "2595,Skylit Midtown Castle,2845,Jennifer,Manhattan,Midtown,40.75362,-73.98377,Entire home/apt,225,1,45,2019-05-21,0.38,2,355\n",
      "3647,THE VILLAGE OF HARLEM....NEW YORK !,4632,Elisabeth,Manhattan,Harlem,40.80902,-73.9419,Private room,150,3,0,,,1,365\n",
      "3831,Cozy Entire Floor of Brownstone,4869,LisaRoxanne,Brooklyn,Clinton Hill,40.68514,-73.95976,Entire home/apt,89,1,270,2019-07-05,4.64,1,194\n"
     ]
    }
   ],
   "source": [
    "!head -5 data/AB_NYC_2019.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by first creating a conda environment with the dependencies and then activating the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env create -f environment.yml\n",
    "!conda activate snakemake-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the workflow in our Snakefile, it includes the steps:\n",
    "- `preprocess`: clean the raw data\n",
    "- `analyze`: generate summary statistics\n",
    "- `visualzie`: create visualizations from the analysis\n",
    "\n",
    "You can view the steps (or the rules) contained in the `Snakefile` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mall\u001b[0m\n",
      "\u001b[32manalyze\u001b[0m\n",
      "\u001b[32mpreprocess\u001b[0m\n",
      "\u001b[32mvisualize\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!snakemake --list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in the slides about defining target files and target rules, notice the first rule our `Snakefile` is `rule all` which defines the 3 output files we want to obtain at the end of our analysis:\n",
    "```python\n",
    "rule all:\n",
    "    input:\n",
    "        \"output/visualizations/price_distribution.png\",\n",
    "        \"output/visualizations/availability_by_neighborhood.png\",\n",
    "        \"output/visualizations/reviews_per_month.png\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the `preprocess` work unit to understand what it does exactly and how we can use Python scripts in a Snakemake workflow. In this step we run the `scripts/preprocess.py` on the input file `data/AB_NYC_2019.csv` and create the `output/cleaned_data.csv` as output. The corresponding rule in our `Snakefile` is:\n",
    "```python\n",
    "rule preprocess:\n",
    "    input:\n",
    "        \"data/AB_NYC_2019.csv\"\n",
    "    output:\n",
    "        \"output/cleaned_data.csv\"\n",
    "    script:\n",
    "        \"scripts/preprocess.py\"\n",
    "```\n",
    "\n",
    "The input file `data/AB_NYC_2019.csv` should be available in your directory already. The Python script `scripts/preprocess.py` is as follows:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load raw data\n",
    "data = pd.read_csv(snakemake.input[0])\n",
    "\n",
    "# Drop rows with missing values in critical columns\n",
    "data = data.dropna(subset=[\"name\", \"host_name\", \"neighbourhood_group\", \"price\"])\n",
    "\n",
    "# Filter out listings with unrealistic prices (e.g., over $1,000)\n",
    "data = data[data[\"price\"] <= 1000]\n",
    "\n",
    "# Normalize column names\n",
    "data.columns = [col.strip().lower().replace(\" \", \"_\") for col in data.columns]\n",
    "\n",
    "# Save cleaned data\n",
    "data.to_csv(snakemake.output[0], index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the `analyze` rule will take the processed data as input, and calculate some basic summary statistics as we can see below:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load cleaned data\n",
    "data = pd.read_csv(snakemake.input[0])\n",
    "\n",
    "# Group data by neighborhood group\n",
    "summary = data.groupby(\"neighbourhood_group\").agg(\n",
    "    avg_price=(\"price\", \"mean\"),\n",
    "    avg_availability=(\"availability_365\", \"mean\"),\n",
    "    total_reviews=(\"reviews_per_month\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# Save summary statistics\n",
    "summary.to_csv(snakemake.output[0], index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can either run the whole pipeline from start to finish, or run individual steps. But be careful, our workflow is sequential, for example the `visualize` step needs the `analyze` step to be run beforehand, and the we need to run the `preprocess` step before we `analyze`. Snakemake is clever enough that if you skip such dependency, it will detect that outputs from previous steps are missing and it'll run the required steps before the particular one you wanted.\n",
    "\n",
    "Before we get started, we can do a \"dry run\" to check the scheduling plan and see if the workflow is defined properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mJob stats:\n",
      "job           count\n",
      "----------  -------\n",
      "all               1\n",
      "analyze           1\n",
      "preprocess        1\n",
      "visualize         1\n",
      "total             4\n",
      "\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Fri Jan 10 11:20:57 2025]\u001b[0m\n",
      "\u001b[32mrule preprocess:\n",
      "    input: data/AB_NYC_2019.csv\n",
      "    output: output/cleaned_data.csv\n",
      "    jobid: 3\n",
      "    reason: Missing output files: output/cleaned_data.csv\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Fri Jan 10 11:20:57 2025]\u001b[0m\n",
      "\u001b[32mrule analyze:\n",
      "    input: output/cleaned_data.csv\n",
      "    output: output/summary.csv\n",
      "    jobid: 2\n",
      "    reason: Missing output files: output/summary.csv; Input files updated by another job: output/cleaned_data.csv\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Fri Jan 10 11:20:57 2025]\u001b[0m\n",
      "\u001b[32mrule visualize:\n",
      "    input: output/summary.csv\n",
      "    output: output/visualizations/price_distribution.png, output/visualizations/availability_by_neighborhood.png, output/visualizations/reviews_per_month.png\n",
      "    jobid: 1\n",
      "    reason: Missing output files: output/visualizations/reviews_per_month.png, output/visualizations/availability_by_neighborhood.png, output/visualizations/price_distribution.png; Input files updated by another job: output/summary.csv\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Fri Jan 10 11:20:57 2025]\u001b[0m\n",
      "\u001b[32mlocalrule all:\n",
      "    input: output/visualizations/price_distribution.png, output/visualizations/availability_by_neighborhood.png, output/visualizations/reviews_per_month.png\n",
      "    jobid: 0\n",
      "    reason: Input files updated by another job: output/visualizations/reviews_per_month.png, output/visualizations/price_distribution.png, output/visualizations/availability_by_neighborhood.png\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[33mJob stats:\n",
      "job           count\n",
      "----------  -------\n",
      "all               1\n",
      "analyze           1\n",
      "preprocess        1\n",
      "visualize         1\n",
      "total             4\n",
      "\u001b[0m\n",
      "\u001b[33mReasons:\n",
      "    (check individual jobs above for details)\n",
      "    input files updated by another job:\n",
      "        all, analyze, visualize\n",
      "    missing output files:\n",
      "        analyze, preprocess, visualize\u001b[0m\n",
      "\u001b[33m\u001b[0m\n",
      "\u001b[33mThis was a dry-run (flag -n). The order of jobs does not reflect the order of execution.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!snakemake -n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For example, try running the code below to `analyze` the data before `preprocess`. Snakemake will take care of it and run both steps for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "\u001b[33mUsing shell: /usr/bin/bash\u001b[0m\n",
      "\u001b[33mProvided cores: 1 (use --cores to define parallelism)\u001b[0m\n",
      "\u001b[33mRules claiming more threads will be scaled down.\u001b[0m\n",
      "\u001b[33mJob stats:\n",
      "job           count\n",
      "----------  -------\n",
      "analyze           1\n",
      "preprocess        1\n",
      "total             2\n",
      "\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Thu Jan  9 12:30:28 2025]\u001b[0m\n",
      "\u001b[32mrule preprocess:\n",
      "    input: data/AB_NYC_2019.csv\n",
      "    output: output/cleaned_data.csv\n",
      "    jobid: 1\n",
      "    reason: Missing output files: output/cleaned_data.csv\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Thu Jan  9 12:30:29 2025]\u001b[0m\n",
      "\u001b[32mFinished job 1.\u001b[0m\n",
      "\u001b[32m1 of 2 steps (50%) done\u001b[0m\n",
      "\u001b[33mSelect jobs to execute...\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Thu Jan  9 12:30:29 2025]\u001b[0m\n",
      "\u001b[32mrule analyze:\n",
      "    input: output/cleaned_data.csv\n",
      "    output: output/summary.csv\n",
      "    jobid: 0\n",
      "    reason: Missing output files: output/summary.csv; Input files updated by another job: output/cleaned_data.csv\n",
      "    resources: tmpdir=/tmp\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\u001b[32m[Thu Jan  9 12:30:30 2025]\u001b[0m\n",
      "\u001b[32mFinished job 0.\u001b[0m\n",
      "\u001b[32m2 of 2 steps (100%) done\u001b[0m\n",
      "\u001b[33mComplete log: .snakemake/log/2025-01-09T123027.514934.snakemake.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!snakemake --cores 1 analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!snakemake --dag --cores 1 | dot -Tpng > airbnb_dag.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](airbnb_dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the status of our workflow as seen by Snakemake, we can use the `--summary` option. It tells you the status of each step, and whether it plans to update any files or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBuilding DAG of jobs...\u001b[0m\n",
      "output_file\tdate\trule\tversion\tlog-file(s)\tstatus\tplan\n",
      "output/visualizations/price_distribution.png\t-\tvisualize\t-\t-\tmissing\tupdate pending\n",
      "output/visualizations/availability_by_neighborhood.png\t-\tvisualize\t-\t-\tmissing\tupdate pending\n",
      "output/visualizations/reviews_per_month.png\t-\tvisualize\t-\t-\tmissing\tupdate pending\n",
      "output/summary.csv\t-\tanalyze\t-\t-\tmissing\tupdate pending\n",
      "output/cleaned_data.csv\t-\tpreprocess\t-\t-\tmissing\tupdate pending\n"
     ]
    }
   ],
   "source": [
    "!snakemake --summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](dag.svg`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakemake-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
